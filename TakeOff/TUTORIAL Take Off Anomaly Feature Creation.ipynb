{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.lib import recfunctions\n",
    "import datetime\n",
    "import numpy.lib.recfunctions as recfn\n",
    "import boto3\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from scipy import stats, fftpack\n",
    "from scipy.stats import kurtosis, skew, iqr, t\n",
    "import pickle\n",
    "from scipy import io as sio\n",
    "s3 = boto3.resource('s3')\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this exercise we look at the aircraft take offs telemetry data to identify anomalies in a bid to identify any abnormality. While commercial flying safety standards are high and well regulated; Take offs can be challenging and engine stalling can be challenge.\n",
    "\n",
    "Here we shall use limited data wrangle on the existing NASA Dashlink Telemetry data to build a  representative feature set of time varying sensor signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLGHTLEN = .75 # Filter out anything that is less than this flight time\n",
    "IN_BKT = 'iia-vault-telemetry-practice-unzipped'\n",
    "SMPL_SIZES= [.01,.01,.1,.1]\n",
    "RANDSEED= 42 # Urban Data Science Myth- Apparently gives best results!!\n",
    "# NUMOFVARS= 30 # Number of variables from the mat files to be selected\n",
    "PRINTATEVERY= 2500 # Key Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Variables\n",
    "\n",
    "There are 186 data variables in the Telemetry data set. What is relevant to aircraft take offs?\n",
    "\n",
    "***To Do:***\n",
    "\n",
    "* Familiarize yourself with the definition of the following variables\n",
    "* Look at the wiki page link <a href=\"https://wiki.iiaweb.com/index.php?title=VAULT/RawData/DASHLink#Format\" target=\"_blank\"> here</a>, look at the description and identify a few variables that may be meaningful for feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['BLAC','CTAC','WS','ALTR']\n",
    "degs = ['AOA1','AOA2','PTCH','ROLL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Read\n",
    "\n",
    "This is a old function with bit of a twist. Sorts the keys by date time in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_bucket_object_keys(bucket_name= IN_BKT):\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    key_list=[]\n",
    "    for key in bucket.objects.all():\n",
    "        key_list.append(key)\n",
    "    sorted(key_list, key = lambda x: int(x.key[16:28]))\n",
    "    return(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = s3_bucket_object_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Prep\n",
    "\n",
    "Picks up a set of non repeating samples as defined by the SMPL_SIZES list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_sets(mat_list, pct_size=SMPL_SIZES):\n",
    "    np.random.seed(seed=RANDSEED)\n",
    "    smpl_sets=[]\n",
    "    for pct in pct_size:\n",
    "        sz = int(pct*len(mat_list))\n",
    "        smpl_sets.append(np.random.choice(mat_list, size=sz, replace=False))\n",
    "    return(smpl_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples= pick_random_sets(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat_file(s3_key, bucketname= IN_BKT):\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucketname, s3_key.key)\n",
    "    inMATFile = obj.get()['Body'].read()\n",
    "    raw_mat = sio.loadmat(BytesIO(inMATFile))\n",
    "    return(raw_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Length \n",
    "\n",
    "Some of the flights are relatively small and may even be trial runs. In order to calculate the length of flight we need to measure length of one of the 186 variable in the mat file with 1 Hz sampling frequency\n",
    "\n",
    "***To Do:***\n",
    "What would be a good candidate variable to use in place of 'SAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flight_len(mat):\n",
    "    return(round(len(mat['SAT']['data'][0][0])/3600,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flght_start_end(mat):\n",
    "    lat = mat['LATP']['data'][0][0]\n",
    "    long = mat['LONP']['data'][0][0]\n",
    "    return(lat[5][0],long[5][0], lat[-5][0],long[-5][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter by flight length\n",
    "\n",
    "This function looks for flights with a specific FLGHTLEN and valid landing gear keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flght_filter(mat, flight_length= FLGHTLEN):\n",
    "    t_len= len(mat['LGDN']['data'][0][0])/3600\n",
    "    valid_lnd= len(np.unique(mat['LGDN']['data'][0][0], return_counts=False))\n",
    "    if t_len >= flight_length and valid_lnd >= 2:\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes for landing gear up and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steady_flight(mat):\n",
    "    lndg= mat['LGUP']['data'][0][0]\n",
    "    lndg_flg_chg= []\n",
    "    for i in range(len(lndg)-1):\n",
    "        if (lndg[i]!=lndg[i+1]):\n",
    "            lndg_flg_chg.append(i+1)\n",
    "    return(lndg_flg_chg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GESD Custom formula\n",
    "\n",
    "Generalized Extreme Studentized Deviate Test\n",
    "\n",
    "In GESD anomalies are progressively evaluated removing the worst offenders and recalculating the test statistics and critical values, or more simply you can say that a range is recalculated after identifying the anomalies in an iterative way.\n",
    "\n",
    "Twitters Anamolize package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew, iqr, t\n",
    "def grubbs_stat(y):\n",
    "    std_dev = np.std(y)\n",
    "    avg_y = np.mean(y)\n",
    "    abs_val_minus_avg = abs(y - avg_y)\n",
    "    max_of_deviations = max(abs_val_minus_avg)\n",
    "    max_ind = np.argmax(abs_val_minus_avg)\n",
    "    Gcal = max_of_deviations/ std_dev\n",
    "#     print(\"Grubbs Statistics Value : {}\".format(Gcal))\n",
    "    return(Gcal, max_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew, iqr, t\n",
    "def calculate_critical_value(size, alpha):\n",
    "    t_dist = t.ppf(1 - alpha / (2 * size), size - 2)\n",
    "    numerator = (size - 1) * np.sqrt(np.square(t_dist))\n",
    "    denominator = np.sqrt(size) * np.sqrt(size - 2 + np.square(t_dist))\n",
    "    critical_value = numerator / denominator\n",
    "#     print(\"Grubbs Critical Value: {}\".format(critical_value))\n",
    "    return(critical_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew, iqr, t\n",
    "def check_G_values(Gs, Gc, inp, max_index, eco= False):\n",
    "    if eco == True:\n",
    "        if Gs > Gc:\n",
    "            print('{} is an outlier. G > G-critical: {:.4f} > {:.4f} \\n'.format(inp[max_index], Gs, Gc))\n",
    "        else:\n",
    "            print('{} is not an outlier. G > G-critical: {:.4f} < {:.4f} \\n'.format(inp[max_index], Gs, Gc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew, iqr, t\n",
    "def ESD_Test(input_series, alpha, max_outliers):\n",
    "    for iterations in range(max_outliers):\n",
    "        Gcritical = calculate_critical_value(len(input_series), alpha)\n",
    "        Gstat, max_index = grubbs_stat(input_series)\n",
    "        check_G_values(Gstat, Gcritical, input_series, max_index)\n",
    "        input_series = np.delete(input_series, max_index)\n",
    "        return([Gcritical, Gstat, max_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous feature\n",
    "\n",
    "## Time aggregate features\n",
    "\n",
    "***To Do:***\n",
    "\n",
    "Can you think of any other metric that might be of interest to represent a time series?\n",
    "Can we take a stab at one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew, iqr, t\n",
    "def conti_stats(mat, continuous, lngd_idx):\n",
    "    var_stats={}\n",
    "    for v in continuous:\n",
    "#         var_stats['key']= v\n",
    "        stats= {}\n",
    "        data =  mat[v]['data'][0][0]\n",
    "        rate = mat[v]['Rate'][0][0][0][0]\n",
    "        data = np.array(data[0: int(lngd_idx[0]*rate): rate])\n",
    "        stats['mean']= np.mean(data)\n",
    "        stats['uniques']= len(np.unique(data))\n",
    "        stats['variance']= np.var(data)\n",
    "        stats['kurt']= kurtosis(data)[0]\n",
    "        stats['skew']= skew(data)[0]\n",
    "        stats['iqr']=iqr(data)\n",
    "        try:\n",
    "            stats['GESD']= ESD_Test(np.squeeze(data).T,0.05,5)\n",
    "        except: \n",
    "            stats['GESD']= [np.NaN, np.NaN, np.NaN]\n",
    "        var_stats[v]= stats\n",
    "    return(var_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degrees Features\n",
    "\n",
    "Degrees are angular measures- neither interval or ratios. One usual way of handling is to convert them into radian and then convert them into SIN/ COS which are obvious ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromdeg(d):\n",
    "    r = d * np.pi / 180.\n",
    "    return(np.cos(r), np.sin(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deg_stats(mat, degs, lngd_idx):\n",
    "    var_stats= {}\n",
    "    for d in degs:\n",
    "        data =  mat[d]['data'][0][0]\n",
    "        rate = mat[d]['Rate'][0][0][0][0]\n",
    "        data = np.array(data[0: int(lngd_idx[0]*rate): rate])\n",
    "        [c,s]= fromdeg(data)\n",
    "        for i, data in enumerate([c,s]):\n",
    "            if i == 0:\n",
    "                var_name = d+'_'+'cos'\n",
    "            else:\n",
    "                var_name = d+'_'+'sin'\n",
    "            stats={}\n",
    "            stats['mean']= np.mean(data)\n",
    "            stats['uniques']= len(np.unique(data))\n",
    "            stats['variance']= np.var(data)\n",
    "            stats['kurt']= kurtosis(data)[0]\n",
    "            stats['skew']= skew(data)[0]\n",
    "            stats['iqr']=iqr(data)\n",
    "            try:\n",
    "                stats['GESD']= ESD_Test(np.squeeze(data).T,0.05,5)\n",
    "            except: \n",
    "                stats['GESD']= [np.NaN, np.NaN, np.NaN]\n",
    "            var_stats[var_name]= stats\n",
    "    return(var_stats) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation Function \n",
    "\n",
    "Loops through all keys, open mat files, extracts data and build the features- One flight at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dictionary(sample_set_num=0):\n",
    "    flghts =0\n",
    "    start = datetime.datetime.now()\n",
    "    print('Main start:', datetime.datetime.now())\n",
    "    mat_list = s3_bucket_object_keys()\n",
    "    print('Key List Read:', datetime.datetime.now())\n",
    "    smpl_set= pick_random_sets(mat_list)\n",
    "    print('Key List Split:', datetime.datetime.now())\n",
    "    s3_keys= samples[sample_set_num]\n",
    "    takeoff_stats= {}\n",
    "    for ind, key in enumerate(s3_keys):\n",
    "        flght_detail= key.key[:28]\n",
    "        train = load_mat_file(key)\n",
    "        stats={}\n",
    "        if flght_filter(train)== True:\n",
    "            take_off_idx= steady_flight(train)\n",
    "#             var_list= select_variables(train, [0,take_off_idx[0]])\n",
    "            flghts +=1\n",
    "            try:\n",
    "                stats['conti']= conti_stats(train, continuous, take_off_idx)\n",
    "                stats['degs']= deg_stats(train, degs, take_off_idx)\n",
    "            except:\n",
    "                continue\n",
    "            takeoff_stats[flght_detail]= stats\n",
    "            if ind% PRINTATEVERY == 0:\n",
    "                print('Processing Key:', key.key, datetime.datetime.now())\n",
    "#                 print(flght_detail, stats)\n",
    "    end = datetime.datetime.now()\n",
    "    print(\"Time Taken to run the function \",str(end-start))\n",
    "    print(\"Total Flights that meet criteria:\", flghts)\n",
    "    return(takeoff_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main start: 2020-09-01 10:26:16.113388\n",
      "Key List Read: 2020-09-01 10:26:37.884483\n",
      "Key List Split: 2020-09-01 10:26:38.323545\n",
      "Processing Key: Flight 659/3/659200201270513.mat 2020-09-01 10:26:38.570340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-fd0cb43dfec9>:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Gcal = max_of_deviations/ std_dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to run the function  0:03:02.711849\n",
      "Total Flights that meet criteria: 572\n"
     ]
    }
   ],
   "source": [
    "takeoff_stats= create_sample_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Management\n",
    "\n",
    "Feature needs to be created in a repeatable and reproducable manner. More over real world problems tend to have a very large feature set and can be tedious to refresh and hence decisions needs to be taken about the freshness of feature which drive the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_write = open(\"flight_stats_tutorial.pickle\", \"wb\")\n",
    "pickle.dump(takeoff_stats, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flight_stats_tutorial.pickle', 'rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flight 655/1/655200104251428\n",
      "{'conti': {'BLAC': {'mean': 0.0042056626175776156, 'uniques': 150, 'variance': 0.0039029210911010284, 'kurt': 8.72461008887516, 'skew': 2.9877432060346774, 'iqr': 0.01660909021803736, 'GESD': [3.9740915882336183, 4.28022956655909, 750]}, 'CTAC': {'mean': -0.00046687632985478847, 'uniques': 57, 'variance': 4.07435156053945e-05, 'kurt': 20.073872164146135, 'skew': -0.031759991144461514, 'iqr': 0.0, 'GESD': [3.9740915882336183, 7.87926749321862, 730]}, 'WS': {'mean': 0.09727480293562014, 'uniques': 6, 'variance': 0.299309185786443, 'kurt': 41.096444799849, 'skew': 6.274601534313071, 'iqr': 0.0, 'GESD': [3.9740915882336183, 8.925735964448865, 738]}, 'ALTR': {'mean': 25.770833333333332, 'uniques': 37, 'variance': 51540.1974826389, 'kurt': 45.28403417425217, 'skew': 6.593205581097803, 'iqr': 48.0, 'GESD': [3.9740915882336183, 9.259915383155764, 764]}}, 'degs': {'AOA1_cos': {'mean': 0.9989343731167027, 'uniques': 127, 'variance': 2.2435556032831233e-06, 'kurt': 28.095997540117608, 'skew': -4.314428848545332, 'iqr': 0.0014337224970482065, 'GESD': [3.9740915882336183, 8.856529746851434, 750]}, 'AOA1_sin': {'mean': 0.012586598746844604, 'uniques': 127, 'variance': 0.0019694521823222957, 'kurt': -0.5656938381923462, 'skew': 0.47405383249015187, 'iqr': 0.08816667444596597, 'GESD': [3.9740915882336183, 3.5176157341721868, 750]}, 'AOA2_cos': {'mean': 0.9988001517925529, 'uniques': 58, 'variance': 1.2790015916850628e-06, 'kurt': 66.22244332585835, 'skew': -7.709144504919696, 'iqr': 0.00013877820323993095, 'GESD': [3.9740915882336183, 10.712441189970823, 750]}, 'AOA2_sin': {'mean': -0.041407988291437, 'uniques': 58, 'variance': 0.0006823562832376623, 'kurt': 36.59978538336264, 'skew': 5.9591048854988635, 'iqr': 0.003064925234378041, 'GESD': [3.9740915882336183, 7.81144043164226, 750]}, 'PTCH_cos': {'mean': 0.9993554492720408, 'uniques': 150, 'variance': 1.3595323815743589e-05, 'kurt': 40.69980582130487, 'skew': -6.466054574620591, 'iqr': 6.0238203227869214e-05, 'GESD': [3.9740915882336183, 7.50392930963524, 767]}, 'PTCH_sin': {'mean': -0.004016018773091509, 'uniques': 150, 'variance': 0.0012589622796759928, 'kurt': 34.060474199814905, 'skew': 5.871282395805151, 'iqr': 0.00690235699957615, 'GESD': [3.9740915882336183, 6.772122689160711, 767]}, 'ROLL_cos': {'mean': 0.9999543312876656, 'uniques': 182, 'variance': 2.650262925731849e-09, 'kurt': 6.055958995117484, 'skew': -2.108496951960994, 'iqr': 4.119471485708459e-05, 'GESD': [3.9740915882336183, 7.03977861204308, 452]}, 'ROLL_sin': {'mean': 0.0037223940459665673, 'uniques': 182, 'variance': 7.747647134093598e-05, 'kurt': -0.04984450750764324, 'skew': -0.5319284817545279, 'iqr': 0.013613504794672696, 'GESD': [3.9740915882336183, 3.6682323309528275, 452]}}}\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for key, value in x.items():\n",
    "    i +=1\n",
    "    if i%500==0:\n",
    "        print(key)\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
