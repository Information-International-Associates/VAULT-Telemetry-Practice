---
title: "Flight 652"
output: gitbook
---
# Analyzing Flight 652


## Load From S3
```{r, cache=TRUE}
library(aws.s3)
library(stringr)

xfun::cache_rds({
#key <- "Flight 652/3/652200201292246.mat"
#key <- "Flight 652/5/652200207061919.mat"
key <- "Flight 652/6/652200210011359.mat"

file_name <- unlist(str_split(key, "/"))[[3]]
bucket <- "iia-vault-telemetry-practice-unzipped"
save_object(key, file=file_name, bucket = bucket)

})
```

## Load Raw MatFile
```{r, cache=TRUE}

# this is the "middle file" on the wiki page https://wiki.iiaweb.com/index.php?title=VAULT/RawData/DASHLink#Format
library(tidyverse)
library(R.matlab)
library(jsonlite) # to save df as json

# when we get around to data exploration
library(ggcorrplot)
library(trelliscopejs)

data <- readMat(file_name)
```

#Define Upsampling Transformation for Raw Matlab Files
```{r, cache=TRUE}
upsample_matfile <- function(data){
  #Determine the maximum number of measurements
  params <- names(data)
  raw_columns <- lapply(params, function(x) data[[x]][1])
  col_lengths <- lapply(raw_columns, function(x) length(unlist(x[1])))
  max_rows <- max(unlist(col_lengths))

  # vector of integers from 1 to max_rows to construct a tibble from
  drop_me_later <- 1:max_rows
  
  # create a tibble
  # we will cbind all 186 normalized vectors to it as columns
  normalized_df <- tibble(drop_me = drop_me_later)
  
  # 186 loops
  for(i in 1:length(data)) {
    # the current column name
    current_index <- params[i]
    
    # the current tibble column
    current_column <- unlist(data[[current_index]][1][1])
    
    # the current column length
    current_length <- length(current_column)
    
    # the current multiplier
    current_multiplier <- max_rows / current_length
    
    # the normalized current column
    current_normalized_column <- rep(current_column, each = current_multiplier)
    
    #remainder <- nrow(normalized_df) - length(current_normalized_column)
    #end_idx <- length(current_normalized_column)
    #start_idx <- end_idx - remainder + 1
    #current_normalized_column <- append(current_normalized_column, current_normalized_column[start_idx:end_idx])
  
    # bind the column  
    normalized_df <- cbind(normalized_df, current_normalized_column  )
  
    # fix the name of the new column (it wnats to name it after the variable)
    names(normalized_df)[names(normalized_df) == "current_normalized_column"] <- current_index
  }
  
  
  
  return(normalized_df)
}

#Upsample the Raw Matfile
normalized_df <- upsample_matfile(data)
# check the names and structure  
print(nrow(normalized_df))
head(normalized_df)
```
## Visualizing the Flight Path
```{r, cache=TRUE}
library(ggmap)
library(maps)
library(mapdata)

#Load a map of the US
usa <- map_data("usa")

#Plot US Map and Flight Path
ggplot() +
  geom_point(data = normalized_df, aes(x = LONP, y = LATP, group = 1), fill = NA, color = "red") +
  geom_polygon(data = usa, aes(x=long, y = lat, group = group), fill = NA, color = "blue")
```

## Adding Timestamp
```{r, cache=TRUE}
library(lubridate)

#Parse the Filename structure for relevant details
tail_number <- substr(file_name, 1,3)
year <- substr(file_name, 4,7)
month <- substr(file_name, 8,9)
day <- substr(file_name, 10,11)
hour <- substr(file_name, 12,13)
minute <- substr(file_name, 14,15)
print(file_name)
date_string <- paste(month, "/",day,"/",year, " ", hour,":",minute, sep="")

#Create Timestamp
start_time <- mdy_hm(date_string)

#Simplistic timestamp (which is inaccuracte based on sampling frequency) that allows you to run anomalize package below.
timestamp <- start_time + seconds(normalized_df$drop_me)

#Warning! The fractional milliseconds causes errors when attempting to run the anomalize package below!! Beware!
#Check max sampling rate
#max_sample_rate <- max(unlist(lapply(params, function(x) unlist(data[[x]][2]))))
#Calculate milliseconds rate from hertz
#milliseconds_rate <- 1000/max_sample_rate
#timestamp <- start_time + milliseconds(sapply(normalized_df$drop_me, function(x) x*milliseconds_rate))
#Allow milliseconds to be viewd
#options(digits.secs = 3)

#Add timestamp feature
normalized_df <- cbind(timestamp, normalized_df)
normalized_df <- as_tibble(normalized_df)
#Remove drop_me index
normalized_df <- select(normalized_df, -drop_me)


head(normalized_df)
```


## Anomaly Detection

### Anomaly Decomposition
```{r, cache=TRUE}
library(tictoc)
library(anomalize)
library(tibbletime)
library(snakecase)

#Update Table Names
names(normalized_df) <- sapply(names(normalized_df), function(x) to_any_case(x, case = "snake"))
#Set Tibble Time Index
normalized_df <- as_tbl_time(normalized_df, index = timestamp)

xfun::cache_rds({
#tic("Analyzing Anomalies")

normalized_df %>% 
  time_decompose(blac, method = "stl", frequency = "auto", trend = "auto") %>%
  anomalize(remainder, method = "gesd", alpha = 0.05, max_anoms = 0.2) %>%
  plot_anomaly_decomposition()

anomalize_cache <- normalized_df %>% 
  time_decompose(blac, method = "stl", frequency = "auto", trend = "auto") %>%
  anomalize(remainder, method = "gesd", alpha = 0.05, max_anoms = 0.2)
#toc()
#tic("Plotting Anomaly Decomposition")
anomalize_cache %>%
  plot_anomaly_decomposition()
#toc()
})
```
```{r}
xfun::cache_rds({
anomalize_cache %>%
  plot_anomaly_decomposition()
})
```

### Save Anomalies to File
```{r, cache=TRUE}
xfun::cache_rds({
anom_df <- anomalize_cache %>%
  time_recompose() %>%
  filter(anomaly == "Yes")

#Write to local

anom_filename <- paste(gsub(".mat", "", file_name), "_blac_anomalies.csv", sep="")
anom_key <- paste("output/",tail_number,"/", anom_filename, sep="")
write.csv(anom_df,anom_filename)
# put local file into S3
put_object(file = anom_filename, object = anom_key, bucket = bucket)
# second save
bucket_out <- "iiaweb-s3-io-practice-bucket"
put_object(file = anom_filename, object = anom_key, bucket = bucket_out)
})

```

### Summary File
```{r}
xfun::cache_rds({
  
number_anomalies <- nrow(anom_df)
rate_anomalies <- nrow(anom_df)/nrow(normalized_df)

summary_df <- data.frame(tail_number, file_name, number_anomalies, rate_anomalies)

#Write to local

summary_filename <- paste(gsub(".mat", "", file_name), "_blac_anomalies_summary.csv", sep="")
summary_key <- paste("summary/",tail_number,"/", summary_filename, sep="")
write.csv(summary_df,summary_filename)
# put local file into S3
put_object(file = summary_filename, object = summary_key, bucket = bucket)
# second save
bucket_out <- "iiaweb-s3-io-practice-bucket"
put_object(file = summary_filename, object = summary_key, bucket = bucket_out)


summary_df
})
```


### Anomaly Plot
```{r, cache=TRUE}
anomalize_cache %>%
  plot_anomaly_decomposition()
```

